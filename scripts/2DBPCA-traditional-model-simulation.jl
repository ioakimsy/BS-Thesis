begin
    using Pkg
    Pkg.activate(".")

    #! Pkg.add() is in 2DBPCA-simulation
    #Pkg.update()

    using Random
    using Statistics
    using DelimitedFiles
    using CSV
    using DataFrames
    using LsqFit
    using BenchmarkTools
    using ProfileView
    using Profile
    using Alert
    using ProgressMeter
end

function initiate_grid(L::Int=8)
	#type should be ::Symbol. Symbol is :xyz
    if L%2 == 1
        L=L+1
    end

    grid = zeros(Int,L,L)
    return grid
end


function generate_next_generation(initial_grid::Matrix{Int},λ::Float64)
    #=
    * Parameter descriptions:
        * This function uses the initial grid of students' states and the spread matrix (λ) to generate the next generate_next_generation
        * The initial grid of student states should be an even-sided matrix with values 0 or 1 (only). This can be generated by the initial_grid() function 
        * λ should be an odd-sided square matrix where each element can have values between 0 and 1

    TODO: adjust the learning probability to account for non-uniform individual learning rate
    TODO: make a probability of unlearning for each student
    TODO: profile the function to identify bottlenecks
    =#

    grid = initial_grid
    next_gen = zeros(Int64, size(grid))
	
    for col in 1:size(initial_grid)[1], row in 1:size(initial_grid)[2]

        if grid[row,col] == 1
            next_gen[row,col] = 1
            continue
        end

        learn_prob = λ

        random_number = rand()

        if random_number <= learn_prob
            next_gen[row,col] = 1
        end
        
    end
    return next_gen
	
end


function simulate_steady_state(class_length, λ, steady_state_tolerance)
    #=
    * simulate_steady_state() keeps generating new generation until steady state is reached
    * The system is considered to be at steady state when there has been no changes in steady_state_tolerance generations
    =#
	initial_class = initiate_grid(class_length)
	generations = [initial_class]
	
	steady_state = false
	num_generations = 1

	while steady_state == false
        next_gen = generate_next_generation(generations[end],λ)	
        push!(generations, next_gen)
        num_generations = num_generations + 1
		
		if generations[end] == generations[max(1,num_generations-steady_state_tolerance)] && num_generations > steady_state_tolerance
			steady_state = true
		end
		
	end

	generations = generations[begin:end-steady_state_tolerance]

	num_generations = length(generations)
	
	return generations, num_generations
end

function generate_directories(lengths::Vector{Int}, Λs::Vector{Float64}, steady_state_tolerance::Int, n_trials::Int)
    folders = ["images", "data"]
    for length in lengths, λ in Λs, folder in folders, trial in 1:n_trials

            mkpath("./output/2D-Binary-PCA/traditional-$(length)/$(λ)/trial_$(trial)/$(folder)")
            
    end
end

function class_simulation(lengths::Vector{Int}, Λs::Vector{Float64}, steady_state_tolerance::Int, n_trials::Int)
    
    max_iters = prod([length(x) for x in [lengths,Λs]]) * n_trials
    prog_bar = Progress(max_iters; showspeed=true)
    
    @. model(x,p) = p[1] * x ^ p[2]

   for trial in 1:n_trials
        for λ₀ in Λs, class_length in lengths#, trial in 1:n_trials

            #println("$seat_config 	$λ₀ 	$class_size 	$trial")

            generations, num_generations = simulate_steady_state(class_length, λ₀, steady_state_tolerance)

            #* Saving raw data
            df_cols = ["Generation $(i)" for i in 1:num_generations]
            df_data = vec.(generations)

            #* row = ith student; column = jth generation
            student_states_df = DataFrame(df_data,df_cols)

            CSV.write("./output/2D-Binary-PCA/traditional-$(class_length)/$(λ₀)/trial_$(trial)/data/2DBPCA-traditional-$(class_length)-$(λ₀)-trial_$(trial)-data.csv",student_states_df)

            #* Normalized to percent of classroom learned
            learned = map(x->sum(x), generations)
            learned = learned ./ maximum(learned; init=1) 
            
            #! makes the output of the maximum 1 when there is no maximum resulting into non-normalized values
            #! it broke once, i do not know why
            
            #* Set up in case need to truncate outliers
            #* Only considers first 25% of the data
            learned_y = learned[1:end-Int64(floor(0.75*num_generations))]
            generation_domain = 1:length(learned_y)

            #* axᵇ where power_coeffs are (a,b)
            fit = curve_fit(model, generation_domain, learned_y, [0.25,2.0], lower = [0.,0.], upper = [1.,5.])
            power_coeffs = coef(fit)
            #standard_errors = stderror(fit)

            #* Writing parameters to CSV file; 
            #! Will break if length of generation is longer than length of fit coeffs (2 for power fit) - verly likely not to happen
            #* power_fit column for the dataframe would be: a, b, σₐ, σᵦ
            fit_params_df = DataFrame(
                learned_per_gen = learned,
                power_fit = [power_coeffs...; [missing for _ in 1:num_generations-length(power_coeffs)]],
            )
   
            CSV.write("./output/2D-Binary-PCA/traditional-$(class_length)/$(λ₀)/trial_$(trial)/data/2DBPCA-traditional-$(class_length)-$(λ₀)-trial_$(trial)-fit_params.csv", fit_params_df)

            ProgressMeter.next!(prog_bar, 
                showvalues = [("λ₀", λ₀), ("Class size", class_length), ("Trial", trial)]
            )
        end
    end
end

#! Main
begin
	# List of parameters
    #TODO: move list of parameters to external file to be read to sync across the scripts
    lengths = [32,48,64,96,128]
	Λs = collect(0.1:0.1:1)
	steady_state_tolerance = 20
	n_trials = 5

	generate_directories(lengths,
        Λs,
        steady_state_tolerance,
        n_trials
	)
	
	@alert class_simulation(lengths,
		Λs,
		steady_state_tolerance,
		n_trials
	)
end