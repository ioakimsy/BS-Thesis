# ! USE BS THESIS ENVIRONMENT (file paths will break if not)
using Pkg
Pkg.activate(".")

#= #= #* Use only when packages aren't in the current environment
Pkg.add("CSV")
Pkg.add("CairoMakie")
Pkg.add("DataFrames")
Pkg.add("CurveFit")
Pkg.add("BenchmarkTools") =#
Pkg.add("ProfileView") =#

#* Use to update packages
# Pkg.update()

using Random
using Statistics
using DelimitedFiles
using CSV
using DataFrames
using CairoMakie
using CurveFit
using BenchmarkTools
using ProfileView
using Profile

function initiate_grid_rand(num_learned::Int=4,L::Int=8)
    grid = zeros(Int,L,L)

    while sum(grid) < num_learned
        grid[rand(1:L),rand(1:L)] = 1
    end

    return grid
end


function initiate_grid(type::String, L::Int=8; n_learned::Int=4)
	#type should be ::Symbol. Symbol is :xyz
    if L%2 == 1
        L=L+1
    end

    grid = zeros(Int,L,L)

    if type == "center"
        grid[L÷2:L÷2+1,L÷2:L÷2+1] .= 1
    elseif type == "outer_corner"
        grid[1,1] = 1
        grid[1,end] = 1
        grid[end,1] = 1
        grid[end,end] = 1 
    elseif type == "inner_corner"
        grid[L÷4,L÷4] = 1
        grid[L÷4,end-L÷4+1] = 1
        grid[end-L÷4+1,L÷4] = 1
        grid[end-L÷4+1,end-L÷4+1] = 1
    elseif type == "random"
        return initiate_grid_rand(n_learned, L)
    else
        error("ERROR: Not a valid initital seating arrangement type")
    end

    return grid
end


function generate_next_generation(initial_grid::Matrix{Int},λ::Matrix{Float64})
    #=
    * Parameter descriptions:
        * This function uses the initial grid of students' states and the propagation matrix (λ) to generate the next generate_next_generation
        * The initial grid of student states should be an even-sided matrix with values 0 or 1 (only). This can be generated by the initial_grid() function 
        * λ should be an odd-sided square matrix where each element can have values between 0 and 1

    TODO: adjust the learning probability to account for non-uniform individual learning rate
    TODO: make a probability of unlearning for each student
    TODO: profile the function to identify bottlenecks
    =#
	border_size = (size(λ)[1]-1)÷2
	
	if size(λ)[1] != size(λ)[2] || size(λ)[1]%2==0
		error("λ not an odd-sided square matrix")
	end
	
    grid = zeros(Int,size(initial_grid)[1]+border_size+1,size(initial_grid)[1]+border_size+1)
	
    grid[border_size+1:end-border_size,border_size+1:end-border_size] .= initial_grid

    next_gen = zeros(size(grid))
	
    for col in border_size+1:size(initial_grid)[1]+border_size, row in border_size+1:size(initial_grid)[2]+border_size

        if grid[row,col] == 1
            next_gen[row,col] = 1
            continue
        end

        neighborhood = grid[row-border_size:row+border_size, col-border_size:col+border_size]
        learn_prob = 1 - prod(1 .- (neighborhood .* λ))

        random_number = rand()

        if random_number <= learn_prob
            next_gen[row,col] = 1
        end
        
    end
    return next_gen[border_size+1:end-border_size,border_size+1:end-border_size]
	
end


function simulate_steady_state(seat_config, class_size, λ, steady_state_tolerance; n_learned=4)
    #=
    * simulate_steady_state() keeps generating new generation until steady state is reached
    * The system is considered to be at steady state when there has been no changes in steady_state_tolerance generations
    =#
	initial_class = initiate_grid(seat_config, class_size; n_learned=n_learned)
	generations = [initial_class]
	
	steady_state = false
	num_generations = 1

	while steady_state == false
			next_gen = generate_next_generation(generations[end],λ)	
			push!(generations, next_gen)
			num_generations = num_generations + 1
		
		if generations[end] == generations[max(1,length(generations)-steady_state_tolerance)] 
			steady_state = true
		end
		
	end

	generations = generations[begin:end-steady_state_tolerance]

	num_generations = length(generations)
	
	return generations, num_generations
end


function generate_directories(sizes::Vector{Int}, seat_configs::Vector{String},Λs::Vector{Float64}, steady_state_tolerance::Int, n_trials::Int; n_learned::Int=4)
    folders = ["images", "data"]
    for seat_config in seat_configs,
        size in sizes,
        λ in Λs,
        folder in folders,
        trial in 1:n_trials

        if seat_config == "random"
            mkpath("./output/2D-Binary-PCA/$(seat_config)-$(size)-$(n_learned)/$(λ)/trial_$(trial)/$(folder)")
        else
            mkpath("./output/2D-Binary-PCA/$(seat_config)-$(size)/$(λ)/trial_$(trial)/$(folder)")
        end
            
    end
end


function class_simulation(sizes::Vector{Int}, seat_configs::Vector{String},Λs::Vector{Float64}, steady_state_tolerance::Int, n_trials::Int; n_learned::Int=4)

   Threads.@threads for trial in 1:n_trials
        for seat_config in seat_configs, λ₀ in Λs, class_size in sizes#, trial in 1:n_trials

            println("$seat_config 	$λ₀ 	$class_size 	$trial")
                
            λ = Float64.( Matrix(
            [ 	λ₀ 		λ₀ 		λ₀;
                λ₀ 		0 		λ₀;
                λ₀ 		λ₀ 		λ₀]
            ))

            generations, num_generations = simulate_steady_state(seat_config, class_size, λ, steady_state_tolerance; n_learned = n_learned)

            #* Saving raw data
            df_cols = ["Generation $(i)" for i in 1:length(generations)]
            df_data = vec.(generations)

            #* row = ith student; column = jth generation
            student_states_df = DataFrame(df_data,df_cols)

            if seat_config == "random"
                CSV.write("./output/2D-Binary-PCA/$(seat_config)-$(class_size)-$(n_learned)/$(λ₀)/trial_$(trial)/data/2DBPCA-$(seat_config)-$(class_size)-$(λ₀)-trial_$(trial)-data.csv",student_states_df)
            else
            CSV.write("./output/2D-Binary-PCA/$(seat_config)-$(class_size)/$(λ₀)/trial_$(trial)/data/2DBPCA-$(seat_config)-$(class_size)-$(λ₀)-trial_$(trial)-data.csv",student_states_df)
            end

            learned = map(x->sum(x), generations)
            learned = learned ./ maximum(learned; init=1) 
            #! makes the output of the maximum 1 when there is no maximum resulting into non-normalized values
            #! it broke once, i do not know why

            #* Set up in case need to truncate outliers
            learned_y = learned[1:end-Int64(floor(0.25*length(learned)))]
            generation_domain = 1:length(learned_y)

            #* axᵇ where power_coeffs are (a,b)
            power_coeffs = power_fit(generation_domain, learned_y)

            #* Writing parameters to CSV file; 
            #! Will break if length of generation is longer than length of fit coeffs (2 for power fit) - verly likely not to happen
            fit_params_df = DataFrame(
                learned_per_gen = learned,
                power_fit = [power_coeffs...; [missing for _ in 1:length(learned)-length(power_coeffs)]],
            )

            if seat_config == "random"
                CSV.write("./output/2D-Binary-PCA/$(seat_config)-$(class_size)-$(n_learned)/$(λ₀)/trial_$(trial)/data/2DBPCA-$(seat_config)-$(class_size)-$(λ₀)-trial_$(trial)-fit_params.csv", fit_params_df)
            else        
                CSV.write("./output/2D-Binary-PCA/$(seat_config)-$(class_size)/$(λ₀)/trial_$(trial)/data/2DBPCA-$(seat_config)-$(class_size)-$(λ₀)-trial_$(trial)-fit_params.csv", fit_params_df)
            end
        end
    end
end

#! Main
function main()
	# List of parameters
	sizes = [128]
	seat_configs = ["random"]
	Λs = [0.25,0.5,0.75]
	steady_state_tolerance = 10
	n_trials = 3
    n_learned = 4

	generate_directories(sizes,
		seat_configs,
		Λs,
		steady_state_tolerance,
		n_trials;
        n_learned = n_learned
	)
	
	class_simulation(sizes,
		seat_configs,
		Λs,
		steady_state_tolerance,
		n_trials;
        n_learned = n_learned
	)
	
end

main()