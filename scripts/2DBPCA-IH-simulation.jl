# ! USE BS THESIS ENVIRONMENT (file paths will break if not)
begin
    using Pkg
    Pkg.activate(".")
    #* Use to update packages
    # Pkg.update()

    println("Loading packages... ")
    using Random
    using Statistics
    using DelimitedFiles
    using CSV
    using DataFrames
    using LsqFit
    using BenchmarkTools
    using ProfileView
    using Profile
    using Alert
    using ProgressMeter
    println("Packages loaded")
end


function initiate_grid_rand(num_learned::Int=4,L::Int=8)
    grid = zeros(Int,L,L)

    while sum(grid) < num_learned
        grid[rand(1:L),rand(1:L)] = 1
    end

    return grid
end


function initiate_grid(type::String, L::Int=8; n_learned::Int=4)
	#type should be ::Symbol. Symbol is :xyz
    if L%2 == 1
        L=L+1
    end

    grid = zeros(Int,L,L)

    if type == "center"
        grid[L÷2:L÷2+1,L÷2:L÷2+1] .= 1
    elseif type == "outer_corner"
        grid[1,1] = 1
        grid[1,end] = 1
        grid[end,1] = 1
        grid[end,end] = 1 
    elseif type == "inner_corner"
        grid[L÷4,L÷4] = 1
        grid[L÷4,end-L÷4+1] = 1
        grid[end-L÷4+1,L÷4] = 1
        grid[end-L÷4+1,end-L÷4+1] = 1
    elseif type == "random"
        return initiate_grid_rand(n_learned, L)
    else
        error("ERROR: Not a valid initital seating arrangement type")
    end

    return grid
end

function initiate_λ_grid(size, λ₀, δ)
    λ = [λ₀ + δ, λ₀ - δ]
    rands = rand(size, size)
    grid = zeros(Float64, size, size)
    grid[rands .< 0.5] .= λ[1]
    grid[rands .>= 0.5] .= λ[2]
    return grid
end

function generate_next_generation(initial_grid::Matrix{Int},ρ::Matrix{Float64}, initial_λ_grid::Matrix{Float64})
    #=
    * Parameter descriptions:
        * This function uses the initial grid of students' states and the spread matrix (ρ) to generate the next generate_next_generation
        * The initial grid of student states should be an even-sided matrix with values 0 or 1 (only). This can be generated by the initial_grid() function 
        * ρ should be an odd-sided square matrix where each element can have values between 0 and 1
        * λ should be the same size as the initial grid with values between 0 and 1

    TODO: adjust the learning probability to account for non-uniform individual learning rate
    TODO: make a probability of unlearning for each student
    TODO: profile the function to identify bottlenecks
    =#
	border_size = (size(ρ)[1]-1)÷2
	
	if size(ρ)[1] != size(ρ)[2] || size(ρ)[1]%2==0
		error("ρ not an odd-sided square matrix")
	end
	
    grid = zeros(Int,size(initial_grid)[1]+border_size+1,size(initial_grid)[1]+border_size+1)
    λ_grid = zeros(Float64,size(initial_grid)[1]+border_size+1,size(initial_grid)[1]+border_size+1)
	
    grid[border_size+1:end-border_size,border_size+1:end-border_size] .= initial_grid
    λ_grid[border_size+1:end-border_size,border_size+1:end-border_size] .= initial_λ_grid

    next_gen = zeros(size(grid))
	
    for col in border_size+1:size(initial_grid)[1]+border_size, row in border_size+1:size(initial_grid)[2]+border_size

        if grid[row,col] == 1
            next_gen[row,col] = 1
            continue
        end

        neighborhood = grid[row-border_size:row+border_size, col-border_size:col+border_size]
        λ_neighborhood = λ_grid[row-border_size:row+border_size, col-border_size:col+border_size]
        
        learn_prob = 1 - prod(1 .- (neighborhood .* ρ .* λ_neighborhood))

        random_number = rand()

        if random_number <= learn_prob
            next_gen[row,col] = 1
        end
        
    end
    return next_gen[border_size+1:end-border_size,border_size+1:end-border_size]
	
end


function simulate_steady_state(seat_config, class_size, ρ, steady_state_tolerance, δλ; n_learned=4, λ₀=0.5)
    #=
    * simulate_steady_state() keeps generating new generation until steady state is reached
    * The system is considered to be at steady state when there has been no changes in steady_state_tolerance generations
    =#
	initial_class = initiate_grid(seat_config, class_size; n_learned=n_learned)
    λ_grid = initiate_λ_grid(class_size, λ₀, δλ)
	generations = [initial_class]
	
	steady_state = false
	num_generations = 1

	while steady_state == false
        next_gen = generate_next_generation(generations[end],ρ, λ_grid)	
        push!(generations, next_gen)
        num_generations = num_generations + 1
		
		if generations[end] == generations[max(1,num_generations-steady_state_tolerance)] && num_generations > steady_state_tolerance
			steady_state = true
		end
		
	end

	generations = generations[begin:end-steady_state_tolerance]

	num_generations = length(generations)
	
	return generations, num_generations, λ_grid
end


function generate_directories(sizes::Vector{Int}, seat_configs::Vector{String},Ρs::Vector{Float64}, δλs::Vector{Float64},  steady_state_tolerance::Int, n_trials::Int; n_learned::Int=4, λ₀=0.5)
    folders = ["images", "data"]
    for seat_config in seat_configs,
        size in sizes,
        ρ in Ρs,
        folder in folders,
        trial in 1+5:n_trials+5,
        δλ in δλs

        if seat_config == "random"
            mkpath("./output/2D-Binary-PCA-IH/$(seat_config)-$(size)-$(n_learned)/$(ρ)-$(λ₀)-$(δλ)/trial_$(trial)/$(folder)")
        else
            mkpath("./output/2D-Binary-PCA-IH/$(seat_config)-$(size)/$(ρ)-$(λ₀)-$(δλ)/trial_$(trial)/$(folder)")
        end
            
    end
end


function class_simulation(sizes::Vector{Int}, seat_configs::Vector{String},Ρs::Vector{Float64}, δλs::Vector{Float64}, steady_state_tolerance::Int, n_trials::Int; n_learned::Int=4, λ₀=0.5)
    
    max_iters = prod([length(x) for x in [sizes,seat_configs,Ρs, δλs]]) * n_trials
    prog_bar = Progress(max_iters; showspeed=true)
    
    @. model(x,p) = p[1] * x ^ p[2]

   for trial in 1+5:n_trials+5
        for seat_config in seat_configs, ρ₀ in Ρs, class_size in sizes, δλ in δλs#, trial in 1:n_trials

            #println("$seat_config 	$ρ₀ 	$class_size 	$trial")
                
            ρ = Float64.( Matrix(
            [ 	ρ₀ 		ρ₀ 		ρ₀;
                ρ₀ 		1 		ρ₀;
                ρ₀ 		ρ₀ 		ρ₀]
            ))

            generations, num_generations, λ_grid = simulate_steady_state(seat_config, class_size, ρ, steady_state_tolerance, δλ; n_learned = n_learned, λ₀=λ₀)

            #* Saving raw data
            df_cols = ["Generation $(i)" for i in 1:num_generations]
            df_data = vec.(generations)

            #* row = ith student; column = jth generation
            student_states_df = DataFrame(df_data,df_cols)

            if seat_config == "random"
                CSV.write("./output/2D-Binary-PCA-IH/$(seat_config)-$(class_size)-$(n_learned)/$(ρ₀)-$(λ₀)-$(δλ)/trial_$(trial)/data/2DBPCAIH-$(seat_config)-$(class_size)-$(n_learned)-$(ρ₀)-$(λ₀)-$(δλ)-trial_$(trial)-data.csv",student_states_df)
                CSV.write("./output/2D-Binary-PCA-IH/$(seat_config)-$(class_size)-$(n_learned)/$(ρ₀)-$(λ₀)-$(δλ)/trial_$(trial)/data/2DBPCAIH-$(seat_config)-$(class_size)-$(n_learned)-$(ρ₀)-$(λ₀)-$(δλ)-trial_$(trial)-lambda_grid.csv", DataFrame(λ_grid, :auto))
            else
                CSV.write("./output/2D-Binary-PCA-IH/$(seat_config)-$(class_size)/$(ρ₀)-$(λ₀)-$(δλ)/trial_$(trial)/data/2DBPCAIH-$(seat_config)-$(class_size)-$(ρ₀)-$(λ₀)-$(δλ)-trial_$(trial)-data.csv",student_states_df)
                CSV.write("./output/2D-Binary-PCA-IH/$(seat_config)-$(class_size)/$(ρ₀)-$(λ₀)-$(δλ)/trial_$(trial)/data/2DBPCAIH-$(seat_config)-$(class_size)-$(ρ₀)-$(λ₀)-$(δλ)-trial_$(trial)-lambda_grid.csv",DataFrame(λ_grid, :auto))
            end

            #* Normalized to percent of classroom learned
            learned = map(x->sum(x), generations)
            learned = learned ./ maximum(learned; init=1) 
            
            #! makes the output of the maximum 1 when there is no maximum resulting into non-normalized values
            #! it broke once, i do not know why
            
            #* Set up in case need to truncate outliers
            learned_y = learned[1:end-Int64(floor(0.5*num_generations))]
            generation_domain = 1:length(learned_y)

            #* axᵇ where power_coeffs are (a,b)
            fit = curve_fit(model, generation_domain, learned_y, [0.25,2.0], lower = [0.,0.], upper = [1.,5.])
            power_coeffs = coef(fit)
            #standard_errors = stderror(fit)

            #* Writing parameters to CSV file; 
            #! Will break if length of generation is longer than length of fit coeffs (2 for power fit) - verly likely not to happen
            #* power_fit column for the dataframe would be: a, b, σₐ, σᵦ
            fit_params_df = DataFrame(
                learned_per_gen = learned,
                power_fit = [power_coeffs...; [missing for _ in 1:num_generations-length(power_coeffs)]],
            )

            if seat_config == "random"
                CSV.write("./output/2D-Binary-PCA-IH/random-$(class_size)-$(n_learned)/$(ρ₀)-$(λ₀)-$(δλ)/trial_$(trial)/data/2DBPCAIH-random-$(class_size)-$(n_learned)-$(ρ₀)-$(λ₀)-$(δλ)-trial_$(trial)-fit_params.csv", fit_params_df)
            else        
                CSV.write("./output/2D-Binary-PCA-IH/$(seat_config)-$(class_size)/$(ρ₀)-$(λ₀)-$(δλ)/trial_$(trial)/data/2DBPCAIH-$(seat_config)-$(class_size)-$(ρ₀)-$(λ₀)-$(δλ)-trial_$(trial)-fit_params.csv", fit_params_df)
            end
            ProgressMeter.next!(prog_bar, 
                showvalues = [("Seat config", seat_config), ("ρ₀", ρ₀), ("Class size", class_size), ("Trial", trial), ("δλ", δλ)]
            )
        end
    end
end

#! Main
begin
	# List of parameters
    #TODO: move list of parameters to external file to be read to sync across the scripts
	# List of parameters
    sizes = [48,96]
	seat_configs = ["outer_corner", "inner_corner", "center", "random"]
	Ρs = collect(0.1:0.1:1)
    δλs = collect(0.0:0.1:0.4)
	steady_state_tolerance = 20
	n_trials = 15
    n_learned = 4
    λ₀ = 0.5

	generate_directories(sizes,
		seat_configs,
		Ρs,
        δλs,
		steady_state_tolerance,
		n_trials;
        n_learned = n_learned,
        λ₀ = λ₀
	)
	
	@alert class_simulation(sizes,
		seat_configs,
		Ρs,
        δλs,
		steady_state_tolerance,
		n_trials;
        n_learned = n_learned,
        λ₀ = λ₀
	)
end
